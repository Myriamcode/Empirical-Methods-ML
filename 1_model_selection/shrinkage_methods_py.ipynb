{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shrinkage Methods\n",
    "\n",
    "The subset selection methods described before involves using OLS to fit a linear model that contains a subset of the predictors. As an alternative, we can fit a model containing _all_ predictors using a technique that constrains or 'regularizes' the coefficient estimates, or equivalently, that _shrinks_ the coefficient estimates towards zero.\n",
    "\n",
    "üíª Previously we were interested in finding the most parsimonious model for the following data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>crime</th>\n",
       "      <th>nox</th>\n",
       "      <th>rooms</th>\n",
       "      <th>dist</th>\n",
       "      <th>radial</th>\n",
       "      <th>proptax</th>\n",
       "      <th>stratio</th>\n",
       "      <th>lowstat</th>\n",
       "      <th>lprice</th>\n",
       "      <th>lnox</th>\n",
       "      <th>lproptax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24000.0</td>\n",
       "      <td>0.006</td>\n",
       "      <td>5.38</td>\n",
       "      <td>6.57</td>\n",
       "      <td>4.09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.600000</td>\n",
       "      <td>15.300000</td>\n",
       "      <td>4.98</td>\n",
       "      <td>10.085810</td>\n",
       "      <td>1.682688</td>\n",
       "      <td>5.690360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21599.0</td>\n",
       "      <td>0.027</td>\n",
       "      <td>4.69</td>\n",
       "      <td>6.42</td>\n",
       "      <td>4.97</td>\n",
       "      <td>2.0</td>\n",
       "      <td>24.200001</td>\n",
       "      <td>17.799999</td>\n",
       "      <td>9.14</td>\n",
       "      <td>9.980402</td>\n",
       "      <td>1.545433</td>\n",
       "      <td>5.488938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34700.0</td>\n",
       "      <td>0.027</td>\n",
       "      <td>4.69</td>\n",
       "      <td>7.18</td>\n",
       "      <td>4.97</td>\n",
       "      <td>2.0</td>\n",
       "      <td>24.200001</td>\n",
       "      <td>17.799999</td>\n",
       "      <td>4.03</td>\n",
       "      <td>10.454500</td>\n",
       "      <td>1.545433</td>\n",
       "      <td>5.488938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33400.0</td>\n",
       "      <td>0.032</td>\n",
       "      <td>4.58</td>\n",
       "      <td>7.00</td>\n",
       "      <td>6.06</td>\n",
       "      <td>3.0</td>\n",
       "      <td>22.200001</td>\n",
       "      <td>18.700001</td>\n",
       "      <td>2.94</td>\n",
       "      <td>10.416310</td>\n",
       "      <td>1.521699</td>\n",
       "      <td>5.402678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36199.0</td>\n",
       "      <td>0.069</td>\n",
       "      <td>4.58</td>\n",
       "      <td>7.15</td>\n",
       "      <td>6.06</td>\n",
       "      <td>3.0</td>\n",
       "      <td>22.200001</td>\n",
       "      <td>18.700001</td>\n",
       "      <td>5.33</td>\n",
       "      <td>10.496790</td>\n",
       "      <td>1.521699</td>\n",
       "      <td>5.402678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     price  crime   nox  rooms  dist  radial    proptax    stratio  lowstat  \\\n",
       "0  24000.0  0.006  5.38   6.57  4.09     1.0  29.600000  15.300000     4.98   \n",
       "1  21599.0  0.027  4.69   6.42  4.97     2.0  24.200001  17.799999     9.14   \n",
       "2  34700.0  0.027  4.69   7.18  4.97     2.0  24.200001  17.799999     4.03   \n",
       "3  33400.0  0.032  4.58   7.00  6.06     3.0  22.200001  18.700001     2.94   \n",
       "4  36199.0  0.069  4.58   7.15  6.06     3.0  22.200001  18.700001     5.33   \n",
       "\n",
       "      lprice      lnox  lproptax  \n",
       "0  10.085810  1.682688  5.690360  \n",
       "1   9.980402  1.545433  5.488938  \n",
       "2  10.454500  1.545433  5.488938  \n",
       "3  10.416310  1.521699  5.402678  \n",
       "4  10.496790  1.521699  5.402678  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# env\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import patsy\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# rd df\n",
    "df = pd.read_stata('http://fmwww.bc.edu/ec-p/data/wooldridge/hprice2.dta')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We actually found out that the chosen model by 'best,' 'forward,' and 'backward' subset search was\n",
    "\n",
    "$$\n",
    "\\texttt{lprice}=\\beta_{0}+\\beta_{1}\\texttt{lnox}+\\beta_{2}\\texttt{lproptax}+\\beta_{3}\\texttt{crime}+\\beta_{4}\\texttt{rooms}+\\beta_{5}\\texttt{dist}+\\beta_{6}\\texttt{radial}+\\beta_{7}\\texttt{stratio}+\\beta_{8}\\texttt{lowstat}+e\n",
    "$$\n",
    "\n",
    "However this model excludes the possibility of interaction terms among the predictors. Instead consider a most complete potential model with _all_ possible cross-products among the regressors _after_ they are re-center at their mean, i.e.,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\texttt{lprice}&=\\beta_{0}+\\beta_{1}\\texttt{lnox}+\\beta_{2}\\texttt{lproptax}+\\beta_{3}\\texttt{crime}+\\beta_{4}\\texttt{rooms}+\\beta_{5}\\texttt{dist}+\\beta_{6}\\texttt{radial}\\\\\n",
    "&+\\beta_{7}\\texttt{stratio}+\\beta_{8}\\texttt{lowstat}+\\beta_{9}(\\texttt{lnox}-\\mu_{\\texttt{lnox}})(\\texttt{lproptax}-\\mu_{\\texttt{lproptax}})\\\\\n",
    "&+\\beta_{10}(\\texttt{lnox}-\\mu_{\\texttt{lnox}})(\\texttt{crime}-\\mu_{\\texttt{crime}})+\\ldots+\\beta_{35}(\\texttt{radial}-\\mu_{\\texttt{radial}})(\\texttt{lowstat}-\\mu_{\\texttt{lowstat}})\\\\\n",
    "&+\\beta_{36}(\\texttt{stratio}-\\mu_{\\texttt{stratio}})(\\texttt{lowstat}-\\mu_{\\texttt{lowstat}})+e.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In this model with over 4 times the original number of predictors, one has that $\\beta_{2}$ represents the constant elasticity of home prices with respect to property tax at the _mean_ value, i.e. $\\mu_{\\texttt{predictor}}$, of the other $\\texttt{predictor}$. Similar interpretations can be given to the coefficients multiplying the other predictors.\n",
    "\n",
    "‚úçüèΩ One can replace $\\mu_{\\texttt{predictor}}$ with other values of the $\\texttt{predictor}$ that may be of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üíª This block will add _all_ cross-products among the _demeaned_ set of predictors to the original data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['price', 'crime', 'nox', 'rooms', 'dist', 'radial', 'proptax', 'stratio', 'lowstat', 'lprice', 'lnox', 'lproptax', 'lnox_dmean', 'lproptax_dmean', 'crime_dmean', 'rooms_dmean', 'dist_dmean', 'radial_dmean', 'stratio_dmean', 'lowstat_dmean']\n"
     ]
    }
   ],
   "source": [
    "# mk demeaned vars\n",
    "tmp = ['lnox','lproptax','crime','rooms','dist','radial','stratio','lowstat']\n",
    "for x in tmp:\n",
    "    df[x+'_dmean'] = df[x] - df[x].mean(skipna = True)\n",
    "print(list(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üíª This code will create both model specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lprice ~ 1 + lnox+lproptax+crime+rooms+dist+radial+stratio+lowstat\n",
      "lprice ~ 1 +  + (lnox_dmean*lproptax_dmean)+(lnox_dmean*crime_dmean)+(lnox_dmean*rooms_dmean)+(lnox_dmean*dist_dmean)+(lnox_dmean*radial_dmean)+(lnox_dmean*stratio_dmean)+(lnox_dmean*lowstat_dmean)+(lproptax_dmean*crime_dmean)+(lproptax_dmean*rooms_dmean)+(lproptax_dmean*dist_dmean)+(lproptax_dmean*radial_dmean)+(lproptax_dmean*stratio_dmean)+(lproptax_dmean*lowstat_dmean)+(crime_dmean*rooms_dmean)+(crime_dmean*dist_dmean)+(crime_dmean*radial_dmean)+(crime_dmean*stratio_dmean)+(crime_dmean*lowstat_dmean)+(rooms_dmean*dist_dmean)+(rooms_dmean*radial_dmean)+(rooms_dmean*stratio_dmean)+(rooms_dmean*lowstat_dmean)+(dist_dmean*radial_dmean)+(dist_dmean*stratio_dmean)+(dist_dmean*lowstat_dmean)+(radial_dmean*stratio_dmean)+(radial_dmean*lowstat_dmean)+(stratio_dmean*lowstat_dmean)\n"
     ]
    }
   ],
   "source": [
    "# mk interact trms\n",
    "tmpstr = []\n",
    "for x in range(len(tmp)):\n",
    "    for y in tmp[x+1:]:\n",
    "        if x != y:\n",
    "            tmpstr.append('('+tmp[x]+'_dmean*'+y+'_dmean'+')')\n",
    "    \n",
    "# mk specifications str\n",
    "f1 = 'lprice ~ 1 + ' + ''.join([x+'+' for x in tmp])[:-1]\n",
    "f2 = 'lprice ~ 1 + ' + ' + ' + ''.join([x+'+' for x in tmpstr])[:-1] \n",
    "\n",
    "# prt specifications str\n",
    "print(f1)\n",
    "print(f2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üíª This code uses the ```patsy``` library to create the corresponding outcome vector and design matrices for both specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1, X1 = patsy.dmatrices(f1, data=df, return_type='dataframe')\n",
    "y2, X2 = patsy.dmatrices(f2, data=df, return_type='dataframe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üíª This code will create the indices for the train (80%) and validation (20%) data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.20, random_state=42)\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Ridge_ Regression\n",
    "\n",
    "The _traditional_ motivation is to reduce the degree of collinearity among the regressors. The _modern_ motivation is regularization of high-dimensional and ill-posed inverse problems.\n",
    "\n",
    "<p style='text-align: right;'> <a href=\"https://en.wikipedia.org/wiki/Tikhonov_regularization\" style=\"color: #cc0000\">Tikhonov Regularization</a></p>\n",
    "\n",
    "#### Traditional Motivation\n",
    "\n",
    "Take a linear regression model $y_{i}=\\mathbf{x}_{i}^{\\prime} \\boldsymbol{\\beta}+e_{i}$. In \"machine learning\" applications the dimension of $\\boldsymbol{\\beta}$ can be very large, and often the regressors are highly correlated. In these cases the least squares estimator may be undefined and/or the $\\mathbf{X}^{\\prime}\\mathbf{X}$ matrix ill-conditioned, which can mean that the least squares coefficient estimates are numerically unreliable. As a numerical solution to this dilemma, Hoerl and Kennard (1970) proposed the ridge regression estimator\n",
    "\n",
    "$$\\widehat{\\boldsymbol{\\beta}}_{\\text {ridge}}=\\left(\\mathbf{X}^{\\prime} \\mathbf{X}+\\lambda \\mathbf{I}_{k}\\right)^{-1} \\mathbf{X}^{\\prime} \\mathbf{y}$$\n",
    "\n",
    "where <span style=\"color:blue\">$\\lambda>0$</span> is a shrinkage parameter and treated later on as a <span style=\"color:blue\">tuning parameter</span>.\n",
    "\n",
    "‚úçüèΩ The ridge regression estimator has the property that it is well-defined and does not suffer from multicollinearity or ill-conditioning so long as $\\lambda>0$. This even holds if $k>n$! That is, the ridge regression estimator can be calculated even when the number of regressors exceeds the sample size.\n",
    "\n",
    "***\n",
    "To see how $\\lambda>0$ ensures that the inverse problem is solved, use the spectral decomposition to write $\\mathbf{X}^{\\prime} \\mathbf{X}=\\mathbf{H}^{\\prime}\\mathbf{D}\\mathbf{H}$ where $\\mathbf{H}$ is orthonormal and $\\mathbf{D}=\\text{diag}\\left\\{r_{1}, \\ldots, r_{k}\\right\\}$ is a diagonal matrix with the eigenvalues $r_{j}$ of $\\mathbf{X}^{\\prime} \\mathbf{X}$ on the diagonal. Let $\\Lambda=\\lambda \\mathbf{I}_{k}$. We can write\n",
    "$$\n",
    "\\mathbf{X}^{\\prime} \\mathbf{X}+\\lambda \\mathbf{I}_{k}=\\mathbf{H}^{\\prime} \\mathbf{D} \\mathbf{H}+\\lambda \\mathbf{H}^{\\prime} \\mathbf{H}=\\mathbf{H}^{\\prime} \\mathbf{D} \\mathbf{H}+\\mathbf{H}^{\\prime} \\Lambda \\mathbf{H}=\\mathbf{H}^{\\prime}(\\mathbf{D}+\\Lambda) \\mathbf{H}\n",
    "$$\n",
    "which has eigenvalues $r_{j}+\\lambda>0$. Thus all eigenvalues are bounded away from zero so $\\mathbf{X}^{\\prime} \\mathbf{X}+\\lambda \\mathbf{I}_{k}$ is full\n",
    "rank and therefore invertible.\n",
    "\n",
    "<p style='text-align: right;'> <a href=\"https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix\" style=\"color: #cc0000\">Spectral Decomposition of a Matrix</a></p>\n",
    "\n",
    "<p style='text-align: right;'> <a href=\"https://en.wikipedia.org/wiki/Orthogonal_matrix\" style=\"color: #cc0000\">Orthonormal Matrix</a></p>\n",
    "\n",
    "<p style='text-align: right;'> <a href=\"https://en.wikipedia.org/wiki/Diagonal_matrix\" style=\"color: #cc0000\">Diagonal Matrix</a></p>\n",
    "\n",
    "***\n",
    "\n",
    "#### Modern Motivation\n",
    "\n",
    "The second motivation is based on penalization. When $\\mathbf{X}^{\\prime}\\mathbf{X}$ is ill-conditioned computing its inverse is \"ill-posed.\" Techniques to deal with ill-posed estimators are called \"regularization\" and a leading method is penalization. Consider the penalized regression criterion (Sum of Squared Errors)\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\\text{SSE}_2(\\mathbf{b}, \\lambda) &=(\\mathbf{y}-\\mathbf{X} \\mathbf{b})^{\\prime}(\\mathbf{y}-\\mathbf{X} \\mathbf{b})+\\lambda \\mathbf{b}^{\\prime} \\mathbf{b} \\\\ &=\\|\\mathbf{y}-\\mathbf{X} \\mathbf{b}\\|_{2}^{2}+\\lambda\\|\\mathbf{b}\\|_{2}^{2} \\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\|\\mathbf{a}\\|_{2}=\\left(\\mathbf{a}^{\\prime} \\mathbf{a}\\right)^{1 / 2}$ is the $L_2$-norm. The minimizer of $\\mathrm{SSE}_2(\\mathbf{b}, \\lambda)$ is a regularized least squares estimator. The first order condition for minimization of $\\mathrm{SSE}_{2}(\\mathbf{b}, \\lambda)$ over $\\mathbf{b}$ is\n",
    "\n",
    "$$\n",
    "-2 \\mathbf{X}^{\\prime}(\\mathbf{y}-X \\mathbf{b})+2 \\lambda \\mathbf{b}=0\n",
    "$$\n",
    "\n",
    "<p style='text-align: right;'> <a href=\"https://en.wikipedia.org/wiki/Euclidean_distance\" style=\"color: #cc0000\">$L_2$-norm</a></p>\n",
    "\n",
    "The solution is $\\widehat{\\boldsymbol{\\beta}}_{\\text {ridge }}$. Thus the regularized (penalized) least squares estimator equals ridge regression. This shows that the ridge regression estimator minimizes the sum of squared errors subject to a penalty on the $L_2$-norm magnitude of the regression coefficient. Penalizing large coefficient vectors keeps the latter from being too large and erratic. _Hence one interpretation of $\\lambda$ is the degree of penalty on the\n",
    "magnitude of the coefficient vector._\n",
    "\n",
    "***\n",
    "Minimization subject to a penalty has a dual representation as constrained minimization. The latter is\n",
    "\n",
    "$$\n",
    "\\min _{\\mathbf{b}}(\\mathbf{y}-\\mathbf{X} \\mathbf{b})^{\\prime}(\\mathbf{y}-\\mathbf{X} \\mathbf{b})\n",
    "$$\n",
    "\n",
    "subject to $\\mathbf{b}^{\\prime} \\mathbf{b} \\leq \\tau$, for some $\\tau>0$. To see the connection, the Lagrangian for the constrained problem is\n",
    "$$\n",
    "\\min _{\\mathbf{b}}(\\mathbf{y}-\\mathbf{X} \\mathbf{b})^{\\prime}(\\mathbf{y}-\\mathbf{X} \\mathbf{b})+\\lambda\\left(\\mathbf{b}^{\\prime} \\mathbf{b}-\\tau\\right),\n",
    "$$\n",
    "\n",
    "where $\\lambda$ is a [Lagrange multiplier](https://en.wikipedia.org/wiki/Lagrange_multiplier).\n",
    "\n",
    "The practical difference between the penalization and constraint problems is that in the first you specify the ridge parameter $\\lambda$ while in the second you specify the constraint $\\tau$. They are connected, since the values of $\\lambda$ and $\\tau$ satisfy the relationship\n",
    "\n",
    "$$\n",
    "\\mathbf{y}^{\\prime} \\mathbf{X}\\left(\\mathbf{X}^{\\prime} \\mathbf{X}+\\lambda \\mathbf{I}_{k}\\right)^{-1}\\left(\\mathbf{X}^{\\prime} \\mathbf{X}+\\lambda \\mathbf{I}_{k}\\right)^{-1} \\mathbf{X}^{\\prime} \\mathbf{y}=\\tau.\n",
    "$$\n",
    "\n",
    "Thus to find $\\lambda$ given $\\tau$ it is sufficient to (numerically) solve this equation.\n",
    "***\n",
    "\n",
    "<img src=\"img/ridge.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "This figure help us visualize the constraint problem as it plots an example in $\\mathbb{R}^2$. The constraint set $\\mathbf{b}^\\prime\\mathbf{b}\\le\\tau$ is displayed as the ball about the origin and the contour sets of the $\\mathrm{SSE}$ are displayed as ellipses. The least squares estimator is the center of the ellipses, while the ridge regression\n",
    "estimator is the point on the circle where the contour is tangent. This shrinks the least squares coefficient towards the zero vector. It shrinks along a trajectory determined by the degree of correlation between the variables. This trajectory is displayed with the dashed lines, marked as 'Ridge path'. This is the sequence of ridge regression coefficients obtained as $\\lambda$ (or $\\tau$) is varied from small to large. When $\\lambda=0$ (or $\\tau$ is large) the ridge estimator equals least squares. For small $\\lambda$ the ridge estimator moves slightly towards the origin by sliding along the ridge of the contour set. As $\\lambda$ increases the ridge estimator takes a more direct path towards the origin.\n",
    "\n",
    "<p style='text-align: right;'> <a href=\"https://en.wikipedia.org/wiki/Real_coordinate_space\" style=\"color: #cc0000\">$\\mathbb{R}^2$</a></p>\n",
    "\n",
    "üíª In these set of commands the Ridge estimators are calculated for a sequence of values of $\\lambda$ and we observe their performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```Ridge()``` function has an alpha argument ($\\lambda$, but with a different name!) that is used to tune the model. We'll generate an array of alpha values ranging from very big to very small, essentially covering the full range of scenarios from the null model containing only the intercept, to the least squares fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.21052632, 0.42105263, 0.63157895, 0.84210526,\n",
       "       1.05263158, 1.26315789, 1.47368421, 1.68421053, 1.89473684,\n",
       "       2.10526316, 2.31578947, 2.52631579, 2.73684211, 2.94736842,\n",
       "       3.15789474, 3.36842105, 3.57894737, 3.78947368, 4.        ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphas = np.linspace(0,4,20)\n",
    "alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge(normalize = True)\n",
    "coefs = []\n",
    "\n",
    "for a in alphas:\n",
    "    ridge.set_params(alpha = a)\n",
    "    ridge.fit(X2, y2)\n",
    "    coefs.append(ridge.coef_)\n",
    "    \n",
    "coefs = pd.DataFrame(np.array(coefs).transpose(2,0,1).reshape(37,-1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect the coefficient estimates to be much smaller when a large value of alpha is used, as compared to when a small value of alpha is used. Let's plot and find out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'lnox_dmean')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "ax = plt.gca()\n",
    "ax.plot(alphas, coefs.iloc[1])\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel(np.array(list(X2))[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we fit a ridge regression model on the training set, and evaluate its MSE on the test set, using  $\\lambda=4$ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept                                           [0.0]\n",
      "lnox_dmean                         [-0.10613291740344942]\n",
      "lproptax_dmean                      [-0.0510464727106796]\n",
      "lnox_dmean:lproptax_dmean          [-0.10176823414703473]\n",
      "crime_dmean                      [-0.0016574298201364986]\n",
      "lnox_dmean:crime_dmean           [-0.0060559407103883515]\n",
      "rooms_dmean                         [0.05297148135731176]\n",
      "lnox_dmean:rooms_dmean            [-0.028368136896556886]\n",
      "dist_dmean                         [0.005765475304370543]\n",
      "lnox_dmean:dist_dmean             [0.0071213723656001005]\n",
      "radial_dmean                     [-0.0015485729574154163]\n",
      "lnox_dmean:radial_dmean          [-0.0033762126669058356]\n",
      "stratio_dmean                     [-0.010009636392996096]\n",
      "lnox_dmean:stratio_dmean           [0.004257507146600561]\n",
      "lowstat_dmean                     [-0.005881157750334575]\n",
      "lnox_dmean:lowstat_dmean          [-0.008941600657595219]\n",
      "lproptax_dmean:crime_dmean       [-0.0021844634703655596]\n",
      "lproptax_dmean:rooms_dmean        [-0.045843401607585835]\n",
      "lproptax_dmean:dist_dmean           [0.01188268182944766]\n",
      "lproptax_dmean:radial_dmean       [-0.002496283296629092]\n",
      "lproptax_dmean:stratio_dmean        [0.00710130110329205]\n",
      "lproptax_dmean:lowstat_dmean      [-0.004043055375539935]\n",
      "crime_dmean:rooms_dmean         [-0.00014324468344522665]\n",
      "crime_dmean:dist_dmean             [0.000491098343231209]\n",
      "crime_dmean:radial_dmean        [-0.00010944133750394048]\n",
      "crime_dmean:stratio_dmean       [-0.00023938866162100835]\n",
      "crime_dmean:lowstat_dmean       [-0.00016048310466454794]\n",
      "rooms_dmean:dist_dmean           [-0.0009653111768134542]\n",
      "rooms_dmean:radial_dmean         [-0.0020433862261259117]\n",
      "rooms_dmean:stratio_dmean         [-0.009708937139967991]\n",
      "rooms_dmean:lowstat_dmean           [-0.0016054033465747]\n",
      "dist_dmean:radial_dmean          [0.00039469262961130275]\n",
      "dist_dmean:stratio_dmean          [0.0007217413210765681]\n",
      "dist_dmean:lowstat_dmean          [0.0009626484418252161]\n",
      "radial_dmean:stratio_dmean        [0.0002691860487364591]\n",
      "radial_dmean:lowstat_dmean      [-0.00020125521186821236]\n",
      "stratio_dmean:lowstat_dmean      [0.00013833252354983986]\n",
      "dtype: object\n",
      "0.06834655379685073\n"
     ]
    }
   ],
   "source": [
    "ridge2 = Ridge(alpha = 4, normalize = True)\n",
    "ridge2.fit(X2_train, y2_train)             # Fit a ridge regression on the training data\n",
    "pred2 = ridge2.predict(X2_test)           # Use this model to predict the test data\n",
    "print(pd.Series(np.array(ridge2.coef_).transpose(1,0).tolist(), index = X2_train.columns)) # Print coefficients\n",
    "print(mean_squared_error(y2_test, pred2))          # Calculate the test MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Intercept', 'lnox_dmean', 'lproptax_dmean',\n",
      "       'lnox_dmean:lproptax_dmean', 'crime_dmean', 'lnox_dmean:crime_dmean',\n",
      "       'rooms_dmean', 'lnox_dmean:rooms_dmean', 'dist_dmean',\n",
      "       'lnox_dmean:dist_dmean', 'radial_dmean', 'lnox_dmean:radial_dmean',\n",
      "       'stratio_dmean', 'lnox_dmean:stratio_dmean', 'lowstat_dmean',\n",
      "       'lnox_dmean:lowstat_dmean', 'lproptax_dmean:crime_dmean',\n",
      "       'lproptax_dmean:rooms_dmean', 'lproptax_dmean:dist_dmean',\n",
      "       'lproptax_dmean:radial_dmean', 'lproptax_dmean:stratio_dmean',\n",
      "       'lproptax_dmean:lowstat_dmean', 'crime_dmean:rooms_dmean',\n",
      "       'crime_dmean:dist_dmean', 'crime_dmean:radial_dmean',\n",
      "       'crime_dmean:stratio_dmean', 'crime_dmean:lowstat_dmean',\n",
      "       'rooms_dmean:dist_dmean', 'rooms_dmean:radial_dmean',\n",
      "       'rooms_dmean:stratio_dmean', 'rooms_dmean:lowstat_dmean',\n",
      "       'dist_dmean:radial_dmean', 'dist_dmean:stratio_dmean',\n",
      "       'dist_dmean:lowstat_dmean', 'radial_dmean:stratio_dmean',\n",
      "       'radial_dmean:lowstat_dmean', 'stratio_dmean:lowstat_dmean'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(X2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
