{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "So far we have been modelling $E[y|\\mathbf{x}]$ through a multiple linear regression model, i.e., $E[y|\\mathbf{x}]=\\mathbf{x}^\\prime\\boldsymbol{\\beta}$. When $y\\in\\{0,1\\}$, the left-hand side of this equality becomes\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E[y|\\mathbf{x}]&=1\\times\\Pr\\{y=1|\\mathbf{x}\\}+0\\times(1-\\Pr\\{y=1|\\mathbf{x}\\})\\\\\n",
    "&=\\Pr\\{y=1|\\mathbf{x}\\}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "<ins>Linear Probability Model</ins>: If we stick to a multiple linear regression model one has\n",
    "\n",
    "$$\n",
    "\\Pr\\{y=1|\\mathbf{x}\\}=\\mathbf{x}^\\prime\\boldsymbol{\\beta}.\n",
    "$$\n",
    "\n",
    "‚ö†Ô∏è The $\\Pr\\{y=1|\\mathbf{x}\\}$ is not necessarily bounded between 0 and 1 as $\\mathbf{x}^\\prime\\boldsymbol{\\beta}$ moves from $-\\infty$ to $+\\infty$.\n",
    "\n",
    "<ins>Logistic Model</ins>: If we were to put the 'index' $\\mathbf{x}^\\prime\\boldsymbol{\\beta}$ inside the [standard logistic function](https://en.wikipedia.org/wiki/Logistic_function) $\\Lambda(u)=[1+\\exp{(-u)}]^{-1}$ and set\n",
    "\n",
    "$$\n",
    "\\Pr\\{y=1|\\mathbf{x}\\}=\\Lambda(\\mathbf{x}^\\prime\\boldsymbol{\\beta}),\n",
    "$$\n",
    "\n",
    "this is known as the *Logistic Link Regression*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-5.1,5.1])\n",
    "axes.set_ylim([-.1,1.1])\n",
    "\n",
    "u = np.linspace(-5.1,5.1,100,endpoint=True)\n",
    "plt.plot(u,1/(1+np.exp(-u)),label='Logistic')\n",
    "plt.plot(u,u,label='Linear')\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.axhline(y=1, color='r', linestyle='--')\n",
    "plt.legend(framealpha=0.0)\n",
    "plt.grid(alpha=0.4,linestyle='--') #Adding a grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple manipulations shows that\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\Pr\\{y=1|\\mathbf{x}\\}&=\\Lambda(\\mathbf{x}^\\prime\\boldsymbol{\\beta})=\\frac{1}{1+\\exp{(-\\mathbf{x}^\\prime\\boldsymbol{\\beta})}}\\\\\n",
    "&=\\frac{\\exp{(\\mathbf{x}^\\prime\\boldsymbol{\\beta})}}{1+\\exp{(\\mathbf{x}^\\prime\\boldsymbol{\\beta})}},\\\\\n",
    "1-\\Pr\\{y=1|\\mathbf{x}\\}&=\\frac{1}{1+\\exp{(\\mathbf{x}^\\prime\\boldsymbol{\\beta})}}\\text{, and therefore}\\\\\n",
    "\\frac{\\Pr\\{y=1|\\mathbf{x}\\}}{1-\\Pr\\{y=1|\\mathbf{x}\\}}&=\\exp{(\\mathbf{x}^\\prime\\boldsymbol{\\beta})}.\\\\\n",
    "\\log{\\left(\\frac{\\Pr\\{y=1|\\mathbf{x}\\}}{1-\\Pr\\{y=1|\\mathbf{x}\\}}\\right)}&=\\mathbf{x}^\\prime\\boldsymbol{\\beta}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "If we define $\\Pr\\{y=1|\\mathbf{x}\\}=p(\\mathbf{x})$ as the probability of 'success,' then $p(\\mathbf{x})/[1-p(\\mathbf{x})]$ is called the [*odds*](https://en.wikipedia.org/wiki/Logit), and can take on *any* value between 0 and $\\infty$. Similarly the log of the odds is called the *log-odds* or *logit*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.linspace(0.01,0.99,100,endpoint=True)\n",
    "plt.plot(p,np.log(p/(1-p)))\n",
    "plt.xlabel('p')\n",
    "plt.ylabel('log(p/(1-p))')\n",
    "plt.grid(alpha=.4,linestyle='--') #Adding a grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ins>Example</ins>: If the probability of success is 0.8, i.e., $p(\\mathbf{x})=0.8$, then the odds equal $\\frac{0.8}{1-0.8}=4$ and one says that the odds of success is 4 to 1. Similarly if the probability of success is 0.5, i.e. $p(\\mathbf{x})=0.5$, then the odds equal $\\frac{0.5}{1-0.5}=1$ and we say that the odds of success is 1 to 1 in this case.\n",
    "\n",
    "**Note**: The parameter $\\beta_j$ represents how an increase of one unit of $x_j$ on average changes the *log-odds*, or equivalently $\\exp{(\\beta_j)}$ represents how an increase of one unit of $x_j$ on average changes the *odds*.\n",
    "\n",
    "üíª We are going to model female labor force participation decision based on various household characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_stata('http://fmwww.bc.edu/ec-p/data/wooldridge/mroz.dta')\n",
    "df.head()\n",
    "tmp = [\"kidslt6\", \"kidsge6\", \"age\", \"educ\",\"exper\", \"hushrs\", \"husage\", \"huseduc\",\"huswage\",\"nwifeinc\",\"mtr\",\"unem\"]\n",
    "for x in tmp:\n",
    "    df[x+'_dmean'] = df[x] - df[x].mean(skipna = True)\n",
    "print(list(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mk interact trms\n",
    "tmpstr = []\n",
    "for x in range(len(tmp)):\n",
    "    for y in tmp[x+1:]:\n",
    "        if x != y:\n",
    "            tmpstr.append('('+tmp[x]+'_dmean*'+y+'_dmean'+')')\n",
    "print(list(tmpstr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mk specifications str\n",
    "f1 = 'inlf ~ 1 + city +' + ''.join([x+'+' for x in tmp])[:-1]\n",
    "f2 = f1 + ' + ' + ''.join([x+'+' for x in tmpstr])[:-1] + ''.join(['-'+x+'_dmean' for x in tmp])\n",
    "\n",
    "# prt specifications str\n",
    "print(f1)\n",
    "print(f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import patsy\n",
    "y1, X1 = patsy.dmatrices(f1, data=df, return_type='dataframe')\n",
    "y2, X2 = patsy.dmatrices(f2, data=df, return_type='dataframe')\n",
    "print(list(X1))\n",
    "print(list(X2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üíª We now partition the augmented data set into the train (75% of the original observations) and test (25% of the original observations) data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.25, random_state=3)\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.25, random_state=3)\n",
    "\n",
    "## printing the response variable for the training set\n",
    "print(y2_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the Regression Coefficients\n",
    "\n",
    "Firstly remember that if we have a *random sample*, then the joint probability of observing the sequence of $n$ ones and zeroes above is given by\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&p(\\mathbf{x}_1)\\cdot(1- p(\\mathbf{x}_2))\\cdot (1-p(\\mathbf{x}_3)\\cdot p(\\mathbf{x}_4)\\cdot p(\\mathbf{x}_5)\\ldots\\\\\n",
    "& = \\prod_{i=1}^{n} p(\\mathbf{x}_i)^{y_i}[1-p(\\mathbf{x}_i)]^{1-y_i}\\\\\n",
    "& = \\prod_{i=1}^{n} \\Lambda(\\mathbf{x}_i^\\prime\\boldsymbol{\\beta})^{y_i}[1-\\Lambda(\\mathbf{x}_i^\\prime\\boldsymbol{\\beta})]^{1-y_i}\\\\\n",
    "&=L(\\boldsymbol{\\beta}|y_1,\\ldots,y_n;\\mathbf{x}_1,\\ldots,\\mathbf{x}_n)\\\\\n",
    "&=:L_n(\\boldsymbol{\\beta}).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The function $L_n(\\boldsymbol{\\beta})$ is called the [*likelihood function*](https://en.wikipedia.org/wiki/Likelihood_function) and if we take the natural logarithm, i.e.,\n",
    "\n",
    "$$\\ell_n(\\boldsymbol{\\beta})=\\log{L_n(\\boldsymbol{\\beta})}$$\n",
    "\n",
    "is called the *log-likelihood function*. Therefore, we can estimate $\\boldsymbol{\\beta}$ by maximizing the *log-likelihood function* as this equivalent as to finding the $\\boldsymbol{\\beta}$ that maximizes the joint probability to observe the sample we have, i.e., $\\widehat{\\boldsymbol{\\beta}}=\\underset{\\mathbf{b}}{\\text{arg max }} \\ell_n(\\mathbf{b})$.\n",
    "\n",
    "üíª We start by estimating the base model, i.e, $\\mathbf{x}$=(1,```city```,```kidslt6```,```kidsge6```,```age```,```educ```,```exper```,```hushrs```,```husage```,```huseduc```,```huswage```,```nwifeinc```,```mtr```,```unem```)$^\\prime$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logit1 = LogisticRegression(fit_intercept=False,max_iter=1000,solver='newton-cg',penalty='none').fit(X1, y1.values.ravel())\n",
    "print(pd.Series(np.exp(logit1.coef_).transpose(1,0).tolist(), index = X1_train.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Interpreting the Logit Coefficients](https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-how-do-i-interpret-odds-ratios-in-logistic-regression/)\n",
    "\n",
    "üíª Holding everything else constant, one extra year of ```educ``` will make the odds of a woman to be working for wages to be 22.61% higher. The odds of a woman working for wages in a ```city``` is 1.033 times that if she were to live in a rural area (non-SMSA) instead. Having 1 extra ```kidslt6``` will reduce the odds of a woman to be working for wages by a factor of 0.321.\n",
    "\n",
    "### Predicted Probabilities\n",
    "\n",
    "üíª Once we have obtained $\\widehat{\\boldsymbol{\\beta}}$ from the training set we can plug-in the features in the validation set and calculate the predicted probabilities to be in the labor force for observations in the validation set. If the predicted probability for observation $j$ in the validaton set is above 0.5 we can predict a 'success,' i.e., $\\widehat{y}_j=1$ and $\\widehat{y}_j=0$ otherwise. Since we *do* observe the actual outcome for said observation, i.e., $y_j$ we can count how many times our model predicted the correct outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit1.score(X1_test, y1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matriz = confusion_matrix(y1_test, logit1.predict(X1_test))\n",
    "print(confusion_matriz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Elastic Net_\n",
    "\n",
    "üíª Now assume that we are interested in fitting a model with a larger set of features, like the one that contains the initial features in the base model *plus* all possible cross-products of the demeaned features excluding ```city```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In this case we can implement the Elastic Net regression as explained before, i.e.,\n",
    "$$\n",
    "\\widehat{\\boldsymbol{\\beta}}=\\underset{\\mathbf{b}}{\\text{arg min }} -\\ell_n(\\mathbf{b}) +\\lambda\\left((1-\\alpha)\\|\\mathbf{b}\\|_{2}^{2}+\\alpha\\|\\mathbf{b}\\|_{1}\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üíª It is recommended to scale the data before fitting and evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X2_train = sc.fit_transform(X2_train.drop('Intercept',1))\n",
    "X2_test = sc.fit_transform(X2_test.drop('Intercept',1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üíª We will train our model using 5-fold cross-validation on the train data set over a combination of 20 values of $\\lambda$, i.e., $\\left{10,5,3.33,2.5,\\cdots,0.526,0.5\\right}$, and 10 values of $\\alpha$, i.e., $\\left{0.1,0.18,\\cdots,0.81,0.9\\right}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "fold = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "searchCV = LogisticRegressionCV(\n",
    "    Cs=list(np.linspace(0.1,2,20,endpoint=True)) #this corresponds to 1/lambda in (7)\n",
    "    ,penalty='elasticnet'\n",
    "    ,l1_ratios=np.linspace(0.1,0.9,10,endpoint=True) #this corresponds to alpha in (7)\n",
    "    ,scoring='accuracy' #proportion of main diag of confusion matrix\n",
    "    ,cv=fold\n",
    "    ,random_state=42\n",
    "    ,max_iter=10000\n",
    "    ,fit_intercept=True\n",
    "    ,solver='saga' #only optimizer available for elasticnet\n",
    "    ,tol=10\n",
    ")\n",
    "logit2_cv = searchCV.fit(X2_train, y2_train.values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üíª We now recover the values $\\widehat{\\lambda}$ and $\\widehat{\\alpha}$ that maximized the average prediction accuracy in the 5 validation data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit2_cv.C_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit2_cv.l1_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üíª We also calculate the proportion of true predictions and confusion matrix using the validation data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit2_cv.score(X2_test, y2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matriz = confusion_matrix(y2_test, logit2_cv.predict(X2_test))\n",
    "print(confusion_matriz)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
